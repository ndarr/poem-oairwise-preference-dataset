{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Retrieve annotations from MTurk batch result file for main dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': '255a6de5-5f57-4302-a1fc-d040ca808c50', 'poem1': \"going before thy path , him ,<br>to thee , away ! thy judgment !<br>stand by the faces i see ,<br>and though it 's not virgil<br>light as a shadow of death .<br>in his choice tiny while he was\", 'poem2': 'look where the grass is gay<br>with summer blossoms , haply there he cowers ;<br>and search , from spray to spray ,<br>the leafy laurel-bowers ,<br>for well he loves the laurels and the flowers .', 'dataset1': 'lstm', 'dataset2': 'gutenberg', 'coherent': ['2', '2', '2'], 'grammatical': ['2', '1', '1'], 'melodious': ['2', '1', '1'], 'moved': ['2', '2', '1'], 'real': ['1', '2', '1'], 'rhyming': ['2', '1', '1'], 'readable': ['1', '2', '2'], 'comprehensible': ['2', '2', '1'], 'intense': ['1', '1', '2'], 'liking': ['1', '1', '2']}\n",
      "850\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "from dataset_utils import DatasetEntry\n",
    "import json\n",
    "from hashlib import sha256\n",
    "\n",
    "batch_file = \"batch_results/Batch_4265208_batch_results_main.csv\"\n",
    "\n",
    "with open(batch_file, \"r\") as csv_file:\n",
    "    csv_reader = csv.reader(csv_file)\n",
    "    i = 0\n",
    "    categories = set()\n",
    "    entries = dict()\n",
    "    \n",
    "    # Skip first line \n",
    "    next(csv_reader)\n",
    "    poemids = []\n",
    "    for row in csv_reader:\n",
    "        # Get entry id\n",
    "        poem_id = row[27]\n",
    "        # Get poem details\n",
    "        poem1 = row[28]\n",
    "        poem2 = row[29]\n",
    "        dataset1 = row[30]\n",
    "        dataset2 = row[31]\n",
    "        \n",
    "        entry = entries.get(poem_id, DatasetEntry(poem_id, poem1, poem2, dataset1, dataset2))\n",
    "        submitted_values = json.loads(str(row[-1]))[0]\n",
    "        entry.update_values(submitted_values)\n",
    "        entries[poem_id] = entry\n",
    "\n",
    "main_entries = entries\n",
    "\n",
    "print(len(main_entries))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1260\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "from dataset_utils import DatasetEntry\n",
    "import json\n",
    "from hashlib import sha256\n",
    "\n",
    "\n",
    "batch_file = \"batch_results/Batch_4278643_batch_results_ext.csv\"\n",
    "\n",
    "with open(batch_file, \"r\") as csv_file:\n",
    "    csv_reader = csv.reader(csv_file)\n",
    "    i = 0\n",
    "    categories = set()\n",
    "    entries = dict()\n",
    "    \n",
    "    # Skip first line \n",
    "    next(csv_reader)\n",
    "    poemids = []\n",
    "    for row in csv_reader:\n",
    "        # Get entry id\n",
    "        poem_id = row[27]\n",
    "        # Get poem details\n",
    "        poem1 = row[28]\n",
    "        poem2 = row[29]\n",
    "        dataset1 = row[30]\n",
    "        dataset2 = row[31]\n",
    "\n",
    "        poem1 = poem1.replace(\"<eol>\", \"<br>\")\n",
    "        poem2 = poem2.replace(\"<eol>\", \"<br>\")\n",
    "        \n",
    "        entry = entries.get(poem_id, DatasetEntry(poem_id, poem1, poem2, dataset1, dataset2))\n",
    "        submitted_values = json.loads(str(row[-1]))[0]\n",
    "        entry.update_values(submitted_values)\n",
    "        entries[poem_id] = entry\n",
    "ext_entries = entries\n",
    "print(len(ext_entries))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2110\n"
     ]
    }
   ],
   "source": [
    "# Put all entries together into one dictionary by their ids\n",
    "cons_entries = {**main_entries, **ext_entries}\n",
    "print(len(cons_entries))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3810\n",
      "['id', 'poem1', 'poem2', 'dataset1', 'dataset2', 'coherent', 'grammatical', 'melodious', 'moved', 'real', 'rhyming', 'readable', 'comprehensible', 'intense', 'liking']\n"
     ]
    }
   ],
   "source": [
    "import jsonpickle\n",
    "from csv import DictWriter\n",
    "\n",
    "split_entries = []\n",
    "for pair_id, entry in cons_entries.items():\n",
    "    poem1 = entry.poem1\n",
    "    poem2 = entry.poem2\n",
    "    dataset1 = entry.dataset1\n",
    "    dataset2 = entry.dataset2\n",
    "    num_annos = len(getattr(entry, \"coherent\"))\n",
    "    for i in range(num_annos):\n",
    "        pair_id_ = f\"{pair_id}_{str(i)}\"\n",
    "        new_entry = DatasetEntry(pair_id_, poem1, poem2, dataset1, dataset2)\n",
    "        for cat in [\"coherent\", \"grammatical\", \"moved\", \"real\", \"rhyming\", \"readable\", \"comprehensible\", \"intense\",\n",
    "                \"liking\", \"melodious\"]:\n",
    "            att = getattr(entry, cat)[i]\n",
    "            setattr(new_entry, cat, att)\n",
    "        split_entries.append(new_entry)\n",
    "\n",
    "print(len(split_entries))\n",
    "split_entries_dict = json.loads(jsonpickle.encode(split_entries, unpicklable=False))\n",
    "\n",
    "fieldnames = list(split_entries_dict[0].keys())\n",
    "with open(\"annotated_datasets/consolidated_batches.csv\", \"w+\") as f:\n",
    "    csv_writer = DictWriter(f, fieldnames)\n",
    "    csv_writer.writeheader()\n",
    "    csv_writer.writerows(split_entries_dict)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Real Poems list\n",
    "For later identifaction of real poems a file containing all real poems is created"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get Real Poems\n",
    "real_poems = []\n",
    "for entry in split_entries:\n",
    "    dataset1 = entry.dataset1\n",
    "    dataset2 = entry.dataset2\n",
    "    \n",
    "    if dataset1 == \"gutenberg\":\n",
    "        real_poems.append(entry.poem1)\n",
    "        \n",
    "    if dataset2 == \"gutenberg\":\n",
    "        real_poems.append(entry.poem2)\n",
    "real_poems = list(set(real_poems))\n",
    "with open(\"real_poems.txt\", \"w+\") as f:\n",
    "    for real_poem in real_poems:\n",
    "        f.write(real_poem + \"\\n\")\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
