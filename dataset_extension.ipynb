{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "batch_path = \"batch_results/Batch_4265208_batch_results_main.csv\"\n",
    "\n",
    "with open(batch_path, \"r\") as csv_file:\n",
    "    csv_reader = csv.reader(csv_file)\n",
    "    csv_lines = list(csv_reader)\n",
    "    \n",
    "\n",
    "\n",
    "# Get all columns but leave out 'Approve' 'Reject' since they are not in the Data yet\n",
    "columns = csv_lines[0][:-2]\n",
    "data_lines = csv_lines[1:]\n",
    "\n",
    "# Read entries from previous annotated batch\n",
    "csv_entries = []\n",
    "for line in data_lines:\n",
    "    entry = dict()\n",
    "    for idx, col in enumerate(columns):\n",
    "        entry[col] = line[idx]\n",
    "    csv_entries.append(entry)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "850\n"
     ]
    }
   ],
   "source": [
    "from dataset_utils import DatasetEntry\n",
    "import json\n",
    "\n",
    "pair_ids = []\n",
    "dataset_entries = {}\n",
    "for entry in csv_entries:\n",
    "    # Get entry id\n",
    "    pair_id = entry['Input.pair_id']\n",
    "    pair_ids.append(pair_id)\n",
    "\n",
    "    # If entry is not yet in dict create it\n",
    "    if pair_id not in dataset_entries.keys():\n",
    "        # Get poem details\n",
    "        poem1 = entry['Input.poem1']\n",
    "        poem2 = entry['Input.poem2']\n",
    "        dataset1 = entry['Input.poem1_dataset']\n",
    "        dataset2 = entry['Input.poem2_dataset']\n",
    "\n",
    "        poem1 = poem1.replace(\"<eol>\", \"<br>\")\n",
    "        poem2 = poem2.replace(\"<eol>\", \"<br>\")\n",
    "\n",
    "        dataset_entry = DatasetEntry(pair_id, poem1, poem2, dataset1, dataset2)\n",
    "        dataset_entries[pair_id] = dataset_entry\n",
    "\n",
    "    dataset_entry = dataset_entries[pair_id]\n",
    "    submitted_values = json.loads(str(entry['Answer.taskAnswers']))[0]\n",
    "    dataset_entry.update_values(submitted_values)  \n",
    "print(len(dataset_entries))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({'gutenberg': 526, 'ngram': 202, 'true_poetry': 180, 'jhamtani': 164, 'deepspeare': 163, 'hafez': 159, 'lstm': 153, 'gpt2': 153})\n",
      "5100\n",
      "Chances: Real(0.5 - Not Real(0.5)\n",
      "Num: Real(105) - Not Real(15)\n",
      "Num unique poems overall: 1469\n",
      "Num chosen poems: 210\n",
      "Occurences data set in base poems: Counter({'gpt2': 15, 'lstm': 15, 'ngram': 15, 'hafez': 15, 'deepspeare': 15, 'jhamtani': 15, 'true_poetry': 15})\n",
      "Num leftovers: 1259\n",
      "210\n",
      "Num new pairs: 1260\n",
      "Occurences data sets as extension poems: Counter({'gutenberg': 603, 'ngram': 102, 'lstm': 97, 'hafez': 97, 'true_poetry': 93, 'gpt2': 93, 'deepspeare': 90, 'jhamtani': 85})\n",
      "Num new poems: 2520\n",
      "Num new unique poems in batch: 974\n",
      "Num real in new pairs: 908\n",
      "Num new pairs: 1260\n",
      "Num old poems 5100\n",
      "Mean num occurencences: 5.187202178352621\n",
      "Occurences Dataset in old and new combined: Counter({'gutenberg': 2760, 'ngram': 812, 'true_poetry': 737, 'hafez': 694, 'jhamtani': 672, 'deepspeare': 664, 'gpt2': 649, 'lstm': 632})\n",
      "Num old pairs: 850\n",
      "850\n",
      "1260\n",
      "\n",
      "Mean unique comparisons: 2.8727025187202178\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import random\n",
    "\n",
    "from collections import Counter\n",
    "from statistics import mean\n",
    "import math\n",
    "from random import shuffle\n",
    "\n",
    "# Get all poems with their respective data set\n",
    "unique_poems = []\n",
    "pairs = []\n",
    "old_poems = []\n",
    "old_dataset_occ = []\n",
    "for _, entry in dataset_entries.items():\n",
    "    tuple1 = (entry.poem1, entry.dataset1)\n",
    "    tuple2 = (entry.poem2, entry.dataset2)\n",
    "    if tuple1 not in unique_poems:\n",
    "        unique_poems.append(tuple1)\n",
    "    if tuple2 not in unique_poems:\n",
    "        unique_poems.append(tuple2)\n",
    "    old_poems.extend([tuple1, tuple2]*3)\n",
    "    old_dataset_occ.extend([entry.dataset1, entry.dataset2])\n",
    "    pairs.append((tuple1, tuple2))\n",
    "\n",
    "print(Counter(old_dataset_occ))\n",
    "\n",
    "old_pairs = pairs\n",
    "least_used_poems = list(set([poem for poem, count in Counter(old_poems).items() if count <= 1]))\n",
    "unique_poems = list(set(old_poems))\n",
    "print(len(old_poems))\n",
    "\n",
    "n = 210\n",
    "chosen_poems = []\n",
    "# Choose 50% real poems\n",
    "temp_poems = []\n",
    "chance_real = 0.5\n",
    "chance_not_real = 1. - chance_real\n",
    "\n",
    "num_real_poems = math.floor(float(n) * chance_real)\n",
    "num_not_real_poems = math.floor(float(n) * chance_not_real / 7)\n",
    "\n",
    "print(f\"Chances: Real({chance_real} - Not Real({chance_not_real})\")\n",
    "print(f\"Num: Real({num_real_poems}) - Not Real({num_not_real_poems})\")\n",
    "\n",
    "\n",
    "poem_pool = unique_poems\n",
    "while len(temp_poems) < num_real_poems:\n",
    "    p = random.choice(poem_pool)\n",
    "    if p not in temp_poems and p[1] == \"gutenberg\":\n",
    "        temp_poems.append(p)\n",
    "chosen_poems.extend(temp_poems)\n",
    "\n",
    "source_datasets = []\n",
    "for model in ['gpt2','lstm','ngram','hafez','deepspeare','jhamtani', 'true_poetry']:\n",
    "    # Choose GPT-2 poems\n",
    "    temp_poems = []\n",
    "    while len(temp_poems) < num_not_real_poems:\n",
    "        p = random.choice(poem_pool)\n",
    "        if p not in temp_poems and p[1] == model:\n",
    "            temp_poems.append(p)\n",
    "            source_datasets.append(model)\n",
    "    chosen_poems.extend(temp_poems)\n",
    "print(f\"Num unique poems overall: {len(unique_poems)}\")\n",
    "print(f\"Num chosen poems: {len(chosen_poems)}\")\n",
    "print(f\"Occurences data set in base poems: {Counter(source_datasets)}\")\n",
    "\n",
    "shuffle(chosen_poems)\n",
    "chosen_poems = chosen_poems[:n]\n",
    "\n",
    "\n",
    "# Create pairs\n",
    "pairs = []\n",
    "leftover_poems = list(set(old_poems) - set(chosen_poems))\n",
    "extension_dataset_list = []\n",
    "print(f\"Num leftovers: {len(leftover_poems)}\")\n",
    "print(len(chosen_poems))\n",
    "\n",
    "\n",
    "corpora = ['gpt2','lstm','ngram','hafez','deepspeare','jhamtani','gutenberg','true_poetry']\n",
    "for poem in chosen_poems:\n",
    "    # Choose 6 other poems differing from the current and not in the list\n",
    "    for i in range(6):\n",
    "        # choose a data set \n",
    "        probs = [chance_not_real / 7.] * 8\n",
    "        probs[6] = chance_real\n",
    "        dataset = random.choices(corpora, probs, k=1)[0]\n",
    "        counter_part = random.choice(leftover_poems)\n",
    "        extension_dataset_list.append(dataset)\n",
    "        while counter_part[1] != dataset or (poem, counter_part) in pairs + old_pairs or (counter_part, poem) in pairs + old_pairs:\n",
    "            counter_part = random.choice(leftover_poems)\n",
    "        # Swap the two poems with a 50% chance\n",
    "        if random.randint(0,1) == 0:\n",
    "            poem, counter_part = counter_part, poem\n",
    "        pairs.append((poem, counter_part))\n",
    "\n",
    "new_pairs = pairs\n",
    "print(f\"Num new pairs: {len(new_pairs)}\")\n",
    "print(f\"Occurences data sets as extension poems: {Counter(extension_dataset_list)}\")\n",
    "        \n",
    "new_unique_poems = []\n",
    "new_poems = []\n",
    "num_real = 0\n",
    "for pair in pairs:\n",
    "    tuple1 = pair[0]\n",
    "    tuple2 = pair[1]\n",
    "    if tuple1[1] == \"gutenberg\" or tuple2[1] == \"gutenberg\":\n",
    "        num_real += 1 \n",
    "    # print(tuple2)\n",
    "    new_poems.extend([tuple1, tuple2])\n",
    "    if tuple1 not in new_unique_poems:\n",
    "        new_unique_poems.append(tuple1)\n",
    "    if tuple2 not in new_unique_poems:\n",
    "        new_unique_poems.append(tuple2)\n",
    "\n",
    "# Get stats about newly created pairs\n",
    "print(f\"Num new poems: {len(new_poems)}\")\n",
    "print(f\"Num new unique poems in batch: {len(new_unique_poems)}\")\n",
    "print(f\"Num real in new pairs: {num_real}\")\n",
    "print(f\"Num new pairs: {len(pairs)}\")\n",
    "\n",
    "\n",
    "print(f\"Num old poems {len(old_poems)}\")\n",
    "old_new_poems = old_poems + new_poems\n",
    "final_occurences = Counter(old_new_poems)\n",
    "# How many times a poem occurs on average\n",
    "print(f\"Mean num occurencences: {mean([count for _, count in final_occurences.items()])}\")\n",
    "\n",
    "\n",
    "print(f\"Occurences Dataset in old and new combined: {Counter([dataset for _, dataset in old_new_poems])}\")\n",
    "\n",
    "\n",
    "# Check how many times each poem is compared to another one\n",
    "print(f\"Num old pairs: {len(old_pairs)}\")\n",
    "old_pairs = set(old_pairs)\n",
    "print(len(old_pairs))\n",
    "new_pairs = set(new_pairs)\n",
    "print(len(new_pairs))\n",
    "comparison_poems = []\n",
    "for pair in (old_pairs | new_pairs):\n",
    "    comparison_poems.append(pair[0])\n",
    "    comparison_poems.append(pair[1])\n",
    "single_comparison_counter = Counter(comparison_poems)\n",
    "print()\n",
    "print(f\"Mean unique comparisons: {mean([count for poem, count in single_comparison_counter.items()])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create actual data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({'gutenberg': 1182, 'hafez': 217, 'ngram': 206, 'true_poetry': 197, 'gpt2': 190, 'jhamtani': 180, 'deepspeare': 175, 'lstm': 173})\n",
      "dict_keys(['pair_id', 'poem1', 'poem2', 'poem1_dataset', 'poem2_dataset', 'question1', 'question1_id', 'question2', 'question2_id', 'question3', 'question3_id', 'question4', 'question4_id', 'question5', 'question5_id'])\n"
     ]
    }
   ],
   "source": [
    "from dataset_utils import PairwisePoemsExt as PairwisePoems, questions\n",
    "from random import shuffle\n",
    "import uuid\n",
    "import jsonpickle\n",
    "\n",
    "amt_pairs = []\n",
    "dataset_list = []\n",
    "for pair in new_pairs:\n",
    "    poem1 = pair[0][0]\n",
    "    dataset1 = pair[0][1]\n",
    "    poem2 = pair[1][0]\n",
    "    dataset2 = pair[1][1]\n",
    "    \n",
    "    dataset_list.extend([dataset1, dataset2])\n",
    "    \n",
    "    question_ids = list(questions.keys())\n",
    "    shuffle(question_ids)\n",
    "    question_set1 = [(questions[q_id], q_id) for q_id in question_ids[:5]]\n",
    "    question_set2 = [(questions[q_id], q_id) for q_id in question_ids[5:]]\n",
    "    \n",
    "    # setup first pair\n",
    "    pair_id = str(uuid.uuid4())\n",
    "    \n",
    "    pair1 = PairwisePoems(pair_id, poem1, poem2, dataset1, dataset2,\n",
    "                         *question_set1[0],\n",
    "                         *question_set1[1],\n",
    "                         *question_set1[2],\n",
    "                         *question_set1[3],\n",
    "                         *question_set1[4])\n",
    "    pair2 = PairwisePoems(pair_id, poem1, poem2, dataset1, dataset2,\n",
    "                         *question_set2[0],\n",
    "                         *question_set2[1],\n",
    "                         *question_set2[2],\n",
    "                         *question_set2[3],\n",
    "                         *question_set2[4])\n",
    "    amt_pairs.extend([pair1, pair2])\n",
    "\n",
    "dataset_occ = Counter(dataset_list)\n",
    "print(dataset_occ)\n",
    "# Write to csv\n",
    "dataset_json = jsonpickle.encode(amt_pairs, unpicklable=False)\n",
    "dataset_dict = json.loads(dataset_json)\n",
    "shuffle(dataset_dict)\n",
    "fieldnames = dataset_dict[0].keys()\n",
    "print(fieldnames)\n",
    "with open(\"csv_dataset_ext.csv\", \"w+\", encoding=\"utf-8\") as csv_file:\n",
    "    writer = csv.DictWriter(csv_file, fieldnames=fieldnames)\n",
    "    writer.writeheader()\n",
    "    writer.writerows(dataset_dict)\n",
    "    \n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}